from fastapi import FastAPI, Request
from pydantic import BaseModel
import numpy as np 
import onnxruntime as ort

app = FastAPI(
    title="QuickServeML Basic API",
    description="Simple ONNX model prediction API",
    version="1.0.0"
)

session = ort.InferenceSession("{{ model_filename }}", providers=["CPUExecutionProvider"])
input_name = session.get_inputs()[0].name

class InputData(BaseModel):
    data: list

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "QuickServeML Basic API",
        "model": "{{ model_filename }}",
        "endpoint": "/predict"
    }

@app.post("/predict")
async def predict(input: InputData):
    """Make a prediction"""
    x = np.array(input.data, dtype=np.float32)
    x = x.reshape({{ input_shape }})
    output = session.run(None, {input_name: x})[0]
    return {"prediction": output.tolist()}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 