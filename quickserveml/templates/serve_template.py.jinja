from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import numpy as np 
import onnxruntime as ort
import logging
from typing import List, Dict, Any
import json

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="QuickServeML API",
    description="Auto-generated FastAPI server for ONNX model inference",
    version="0.1.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load the ONNX model
try:
    session = ort.InferenceSession("{{ model_filename }}", providers=["CPUExecutionProvider"])
    input_name = session.get_inputs()[0].name
    output_names = [output.name for output in session.get_outputs()]
    logger.info(f"Model loaded successfully: {{ model_filename }}")
    logger.info(f"Input name: {input_name}")
    logger.info(f"Output names: {output_names}")
except Exception as e:
    logger.error(f"Failed to load model: {e}")
    raise

class InputData(BaseModel):
    data: List[float] = Field(..., description="Input data as a flat list of floats")
    
    class Config:
        schema_extra = {
            "example": {
                "data": [0.0] * ({{ input_shape | length }})
            }
        }

class PredictionResponse(BaseModel):
    prediction: List[float] = Field(..., description="Model prediction output")
    input_shape: List[int] = Field(..., description="Reshaped input shape")
    output_shape: List[int] = Field(..., description="Output shape")

@app.get("/")
async def root():
    """Root endpoint with API information."""
    return {
        "message": "QuickServeML API is running!",
        "model": "{{ model_filename }}",
        "endpoints": {
            "predict": "/predict",
            "docs": "/docs",
            "health": "/health"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "model": "{{ model_filename }}"}

@app.get("/model-info")
async def model_info():
    """Get model information."""
    inputs = session.get_inputs()
    outputs = session.get_outputs()
    
    return {
        "model_file": "{{ model_filename }}",
        "inputs": [
            {
                "name": inp.name,
                "shape": inp.shape,
                "type": str(inp.type)
            } for inp in inputs
        ],
        "outputs": [
            {
                "name": out.name,
                "shape": out.shape,
                "type": str(out.type)
            } for out in outputs
        ]
    }

@app.post("/predict", response_model=PredictionResponse)
async def predict(input_data: InputData):
    """
    Make a prediction using the loaded ONNX model.
    
    Args:
        input_data: Input data as a flat list of floats
        
    Returns:
        PredictionResponse: Model prediction with metadata
    """
    try:
        # Convert input to numpy array
        x = np.array(input_data.data, dtype=np.float32)
        
        # Reshape to expected input shape
        expected_shape = {{ input_shape }}
        x = x.reshape(expected_shape)
        
        logger.info(f"Input shape: {x.shape}")
        
        # Run inference
        outputs = session.run(output_names, {input_name: x})
        prediction = outputs[0]  # Take first output
        
        logger.info(f"Prediction successful, output shape: {prediction.shape}")
        
        return PredictionResponse(
            prediction=prediction.flatten().tolist(),
            input_shape=list(x.shape),
            output_shape=list(prediction.shape)
        )
        
    except ValueError as e:
        logger.error(f"Input validation error: {e}")
        raise HTTPException(
            status_code=400, 
            detail=f"Input validation error: {str(e)}"
        )
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Prediction failed: {str(e)}"
        )

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Global exception handler."""
    logger.error(f"Unhandled exception: {exc}")
    return {
        "error": "Internal server error",
        "detail": str(exc) if logger.level <= logging.DEBUG else "An unexpected error occurred"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)


